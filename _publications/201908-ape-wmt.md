---
title: "Unbabel's Submission to the WMT2019 APE Shared Task: BERT-based Encoder-Decoder for Automatic Post-Editing"
collection: publications
permalink: /publication/201908-ape-wmt
authors: 'António V. Lopes, M. Amin Farajian, Gonçalo M. Correia, Jonay Trenous, André F.T. Martins'
conference: 'In Proceedings of WMT'
conference_year: '2019'
arxiv_link: 'https://arxiv.org/abs/1905.13068'
code_link: 'https://github.com/deep-spin/OpenNMT-APE'
abstract: "This paper describes Unbabel's submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pre-trained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder (BED) model in which a single pretrained BERT encoder receives both the source src and machine translation tgt strings. Furthermore, we explore a conservativeness factor to constrain the APE system to perform fewer edits. As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservativeness penalty improves significantly the translations of a strong Neural Machine Translation system by and in terms of TER and BLEU, respectively."
bibtex: "@inproceedings{Lopes2019,
author = {Lopes, António V. and Farajian, M. Amin and Correia, Gonçalo M. and Trenous, Jonay and Martins, André F. T.},
booktitle = {Proceedings of WMT19},
title = Unbabel's Submission to the WMT2019 APE Shared Task: BERT-based Encoder-Decoder for Automatic Post-Editing,
year = {2019}
}"
comment: "Winner of the Shared Task!"
---